TRAINING:
    print_freq: 100
    label_smoothing: 0.1
    num_updates: 100000
    ckpt_path: '/data/standard_datasets/meta_dataset/pytorch_checkpoints/checkpoints/'
    mae_ckpt_path: '/data/standard_datasets/meta_dataset/mae_models/'
    seed: 2021
    visdom_port: 0
    eval_freq: 500
    loss: 'xent'
    focal_gamma: 3
    debug: False

AUGMENTATIONS:
    beta: 1.0
    cutmix_prob: 1.0
    augmentation: 'none'

MODEL:
    arch: 'wideres2810'
    use_fc: True
    timm_name: 'NONE'

OPTIM:
    gamma: 0.1
    lr: 0.1
    lr_stepsize: 30
    nesterov: False
    weight_decay: 0.0001
    optimizer_name: 'SGD'
    scheduler: 'multi_step'

DATA:
     # Data config
    image_size: 84
    base_source: 'ilsvrc_2012'
    val_source: 'ilsvrc_2012'
    test_source: 'ilsvrc_2012'
    batch_size: 256
    path: '/data/standard_datasets/meta_dataset/records'
    split: 'train'
    num_workers: 8
    train_transforms: ['random_resized_crop', 'jitter', 'random_flip', 'to_tensor', 'normalize']
    test_transforms: ['resize', 'center_crop', 'to_tensor', 'normalize']
    num_unique_descriptions: 0
    shuffle: True
    loader_version: 'pytorch'

EVAL-GENERAL:
    eval_metrics: ['Acc']
    plot_freq: 10
    model_tag: 'best'
    eval_mode: 'test'
    val_episodes: 100
    val_batch_size: 1
    iter: 1
    extract_batch_size: 0  # set to >0 to batch feature extraction (save memory) at inference
    center_features: False


EVAL-VISU:
    res_path: 'results/'
    max_s_visu: 2
    max_q_visu: 3
    max_class_visu: 5
    visu: False
    visu_freq: 10

EVAL-EPISODES:
    num_ways: 10
    num_support: 10
    num_query: 15
    min_ways: 5
    max_ways_upper_bound: 50
    max_num_query: 10
    max_support_set_size: 500
    max_support_size_contrib_per_class: 100
    min_log_weight: -0.69314718055994529
    max_log_weight: 0.69314718055994529
    ignore_dag_ontology: False
    ignore_bilevel_ontology: True
    ignore_hierarchy_probability: 0
    min_examples_in_class: 0

METHOD:
    episodic_training: False

DISTRIBUTED:
    gpus: [1]
